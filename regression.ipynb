{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-11T08:41:04.243164812Z",
     "start_time": "2023-10-11T08:41:00.222666774Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import sklearn.manifold as skm\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, scale, minmax_scale\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from project.mlp import MLP, CategMLP, AttentionMLP\n",
    "from project.autoencoder import  AutoEncoder, DAE\n",
    "from project.custom_dataset import CustomDataset, CategoricalDataset\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxime/anaconda3/envs/Test_project/lib/python3.8/site-packages/sklearn/utils/validation.py:787: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/home/maxime/anaconda3/envs/Test_project/lib/python3.8/site-packages/sklearn/utils/validation.py:787: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/playlist_2010to2022.csv\").dropna()\n",
    "df = df.drop(columns=['playlist_url', 'track_id', 'artist_id'], axis=1)\n",
    "mlb = MultiLabelBinarizer(sparse_output=True)\n",
    "genres = pd.DataFrame.sparse.from_spmatrix(mlb.fit_transform(df.artist_genres.str.strip(\"[]\").str.split(', ')),\n",
    "                                           columns=mlb.classes_, index=df.index).drop(\"\", axis=1)\n",
    "genres.columns = genres.columns.str.strip(\"'\")\n",
    "df = df.drop(columns=['artist_genres'], axis=1)\n",
    "ohe_year = pd.get_dummies(df.year).astype(int)\n",
    "target = df.pop(\"track_popularity\")\n",
    "df = df.drop(columns=['track_name', 'year', 'artist_name', 'artist_popularity', 'album'], axis=1)\n",
    "df.columns = df.columns.astype(str)\n",
    "#split in train and test 0.8 t 0.2 \n",
    "\n",
    "full = pd.concat([df, genres, ohe_year], axis=1)\n",
    "full.columns = full.columns.astype(str)\n",
    "\n",
    "train = full.sample(frac=0.8, random_state=0)\n",
    "test = full.drop(train.index) \n",
    "train_idx = train.index\n",
    "train_target = target[train.index].values\n",
    "test_target = target[test.index].values\n",
    "\n",
    "train, test = scale(train), scale(test)\n",
    "\n",
    "\n",
    "feature_year_no_genre = pd.concat([df, ohe_year], axis=1)\n",
    "feature_year_no_genre.columns = feature_year_no_genre.columns.astype(str)\n",
    "\n",
    "train_no_genre = feature_year_no_genre.loc[train_idx].values\n",
    "test_no_genre = feature_year_no_genre.drop(train_idx).values\n",
    "train_no_genre, test_no_genre = scale(train_no_genre), scale(test_no_genre)\n",
    "\n",
    "\n",
    "train_no_genre_no_ohe = df.loc[train_idx].values\n",
    "test_no_genre_no_ohe = df.drop(train_idx).values\n",
    "train_no_genre_no_ohe, test_no_genre_no_ohe = scale(train_no_genre_no_ohe), scale(test_no_genre_no_ohe)\n",
    "\n",
    "train_genre_no_year = pd.concat([df, genres], axis=1)\n",
    "train_genre_no_year.columns = train_genre_no_year.columns.astype(str)\n",
    "train_genre_no_year = train_genre_no_year.loc[train_idx]\n",
    "test_genre_no_year = pd.concat([df, genres], axis=1).drop(train_idx).values\n",
    "train_genre_no_year, test_genre_no_year = scale(train_genre_no_year.values), scale(test_genre_no_year)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T08:41:04.759141886Z",
     "start_time": "2023-10-11T08:41:04.248935109Z"
    }
   },
   "id": "cbf5daa025ac5289"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test_train_error(model, train, train_target, test, test_target):\n",
    "    train_error = ((model.predict(train) - train_target)**2).sum()\n",
    "    test_error = ((model.predict(test) - test_target)**2).sum()\n",
    "    print(\"Train error : \", train_error, \" ; test error : \", test_error)\n",
    "    return train_error, test_error\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88833f70b78aab1b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Linear Ridge and Lasso regression using the year and genre as one hot encoded vector\")\n",
    "#use test_train_error and store result in a dictionnary\n",
    "train_dict = defaultdict(lambda: defaultdict(dict))\n",
    "test_dict = defaultdict(lambda: defaultdict(dict))\n",
    "train_dict[\"Linear\"][\"ohe_genre\"][\"ohe_year\"], test_dict[\"Linear\"][\"ohe_genre\"][\"ohe_year\"] = test_train_error(LinearRegression().fit(train, train_target), train, train_target, test, test_target)\n",
    "train_dict[\"Ridge\"][\"ohe_genre\"][\"ohe_year\"], test_dict[\"Ridge\"][\"ohe_genre\"][\"ohe_year\"] = test_train_error(Ridge().fit(train, train_target), train, train_target, test, test_target)\n",
    "train_dict[\"Lasso\"][\"ohe_genre\"][\"ohe_year\"], test_dict[\"Lasso\"][\"ohe_genre\"][\"ohe_year\"] = test_train_error(Lasso().fit(train, train_target), train, train_target, test, test_target)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d023453de413610d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Linear Ridge and Lasso regression using the year as one hot encoded vector, but no genre\")\n",
    "train_dict[\"Linear\"][\"no_genre\"][\"ohe_year\"], test_dict[\"Linear\"][\"no_genre\"][\"ohe_year\"] = test_train_error(LinearRegression().fit(train_no_genre, train_target), train_no_genre, train_target, test_no_genre, test_target)\n",
    "train_dict[\"Ridge\"][\"no_genre\"][\"ohe_year\"], test_dict[\"Ridge\"][\"no_genre\"][\"ohe_year\"] = test_train_error(Ridge().fit(train_no_genre, train_target), train_no_genre, train_target, test_no_genre, test_target)\n",
    "train_dict[\"Lasso\"][\"no_genre\"][\"ohe_year\"], test_dict[\"Lasso\"][\"no_genre\"][\"ohe_year\"] = test_train_error(Lasso().fit(train_no_genre, train_target), train_no_genre, train_target, test_no_genre, test_target)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72995fa8758c8a3c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Linear Ridge and Lasso regression using the year as int, and no genre\")\n",
    "train_dict[\"Linear\"][\"no_genre\"][\"no_ohe_year\"], test_dict[\"Linear\"][\"no_genre\"][\"no_ohe_year\"] = test_train_error(LinearRegression().fit(train_no_genre_no_ohe, train_target), train_no_genre_no_ohe, train_target, test_no_genre_no_ohe, test_target)\n",
    "train_dict[\"Ridge\"][\"no_genre\"][\"no_ohe_year\"], test_dict[\"Ridge\"][\"no_genre\"][\"no_ohe_year\"] = test_train_error(Ridge().fit(train_no_genre_no_ohe, train_target), train_no_genre_no_ohe, train_target, test_no_genre_no_ohe, test_target)\n",
    "train_dict[\"Lasso\"][\"no_genre\"][\"no_ohe_year\"], test_dict[\"Lasso\"][\"no_genre\"][\"no_ohe_year\"] = test_train_error(Lasso().fit(train_no_genre_no_ohe, train_target), train_no_genre_no_ohe, train_target, test_no_genre_no_ohe, test_target)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "753a9c93885c233e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Linear Ridge and Lasso regression using the year as int, and one hot encoed genre\")\n",
    "train_dict[\"Linear\"][\"ohe_genre\"][\"no_ohe_year\"], test_dict[\"Linear\"][\"ohe_genre\"][\"no_ohe_year\"] = test_train_error(LinearRegression().fit(train_genre_no_year, train_target), train_genre_no_year, train_target, test_genre_no_year, test_target)\n",
    "train_dict[\"Ridge\"][\"ohe_genre\"][\"no_ohe_year\"], test_dict[\"Ridge\"][\"ohe_genre\"][\"no_ohe_year\"] = test_train_error(Ridge().fit(train_genre_no_year, train_target), train_genre_no_year, train_target, test_genre_no_year, test_target)\n",
    "train_dict[\"Lasso\"][\"ohe_genre\"][\"no_ohe_year\"], test_dict[\"Lasso\"][\"ohe_genre\"][\"no_ohe_year\"] = test_train_error(Lasso().fit(train_genre_no_year, train_target), train_genre_no_year, train_target, test_genre_no_year, test_target)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b47a7c2ba776a601"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# format train and test dict as a dataframe\n",
    "train_df = pd.DataFrame(train_dict)\n",
    "test_df = pd.DataFrame(test_dict)\n",
    "train_df = train_df.stack(level=0).reset_index().rename(columns={\"level_0\": \"genre\", \"level_1\": \"model\", \"level_2\": \"year\", 0: \"train_error\"})\n",
    "train_df = pd.concat([train_df, pd.DataFrame(list(train_df.train_error))], axis=1).drop(columns=[\"train_error\"]).melt([\"model\", \"genre\"]).rename(columns={\"variable\": \"encoded_year\"})\n",
    "\n",
    "test_df = test_df.stack(level=0).reset_index().rename(columns={\"level_0\": \"genre\", \"level_1\": \"model\", \"level_2\": \"year\", 0: \"test_error\"})\n",
    "test_df = pd.concat([test_df, pd.DataFrame(list(test_df.test_error))], axis=1).drop(columns=[\"test_error\"]).melt([\"model\", \"genre\"]).rename(columns={\"variable\": \"encoded_year\"})\n",
    "\n",
    "# merge two df\n",
    "error_df = pd.merge(train_df, test_df, on=[\"model\", \"genre\", \"encoded_year\"]).rename(columns={\"value_x\": \"train_error\", \"value_y\": \"test_error\"})\n",
    "error_df = error_df.melt([\"model\", \"genre\", \"encoded_year\"], [\"train_error\", \"test_error\"], \"error_type\").rename(columns={\"error_type\": \"error_type\", \"value\": \"error\"})\n",
    "\n",
    "error_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f914fddbd3fa0e7b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def signif(x, p):\n",
    "    x = np.asarray(x)\n",
    "    x_positive = np.where(np.isfinite(x) & (x != 0), np.abs(x), 10**(p-1))\n",
    "    mags = 10 ** (p - 1 - np.floor(np.log10(x_positive)))\n",
    "    return np.round(x * mags) / mags\n",
    "\n",
    "error_df[\"error\"] = signif(error_df.error, 2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97f9aff69a6d70f4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = px.bar(error_df, x=\"model\", y=\"error\", color=\"error_type\", facet_row=\"genre\", facet_col=\"encoded_year\", barmode=\"group\", title=\"Train and test error for different models and different encoding of the year and genre\", log_y=True, text_auto=True)\n",
    "#round values to two significant digits \n",
    "fig.update_traces(textposition='outside')\n",
    "fig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide')\n",
    "#make figure taller\n",
    "fig.update_layout(height=1200)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42243454e553981"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We obtain the best result by on hot encoding the genres and the year, with a Ridge regression. Here it is interesting to see that one hot encoding the year helps the model, as it removes information by removing the ordinality of the feature (the year 2020 is a higher number than the year 2019) \n",
    "The genre seems to be quite important here as it the feature the has the most impact on lowering the error rate."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63da33981bd3521f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the remainder of this notebook we will use an MLP to predict track popularity and will observe the impact of the \"categorical embedding\" technique on the error rate. \n",
    "Factorization machines are also an interesting technique to use for this kind of problem, and might use them later"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "460619216a6cafca"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# create test and train dataloader from the dataset\n",
    "train, test = train.astype(np.float32), test.astype(np.float32)\n",
    "train_dataset = CustomDataset(train, train_target)\n",
    "test_dataset =  CustomDataset(test, test_target)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=8)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=8)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T08:44:25.756497261Z",
     "start_time": "2023-10-11T08:44:25.709902197Z"
    }
   },
   "id": "6a1184cc1587e34f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# initialize mlp and create a gridsearch to find the best parameters recording test and train error for each combination of parameters\n",
    "lr = [0.001, 0.01, 0.1]\n",
    "hidden_layer_width = [32, 64, 128]\n",
    "hidden_layer_depth = [1, 2, 4, 8]\n",
    "epochs = [20, 50,100]\n",
    "dropout = [0.0, 0.2, 0.5]\n",
    "grid = np.array(np.meshgrid(lr, hidden_layer_width, hidden_layer_depth, epochs, dropout)).T.reshape(-1, 5)\n",
    "grid = pd.DataFrame(grid, columns=[\"lr\", \"hidden_layer_width\", \"hidden_layer_depth\", \"epochs\", \"dropout\"])\n",
    "grid[\"train_error\"] = np.nan\n",
    "grid[\"test_error\"] = np.nan\n",
    "grid[[\"hidden_layer_width\", \"hidden_layer_depth\", \"epochs\"]] = grid[[\"hidden_layer_width\", \"hidden_layer_depth\", \"epochs\"]].astype(int)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a726d50793e7232c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b3644a79c671a3be"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# go through the grid and train the model for each combination of parameters\n",
    "for i in tqdm(range(len(grid))):\n",
    "    layers = [grid.hidden_layer_width[i]]*grid.hidden_layer_depth[i]\n",
    "    model = MLP(input_dim=train.shape[1], output_dim=1, hidden_layers=layers, lr=grid.lr[i], dropout=grid.dropout[i])\n",
    "    trainer = Trainer(max_epochs=int(grid.epochs[i]), enable_progress_bar=False, enable_model_summary=False)\n",
    "    trainer.fit(model, train_dataloader, test_dataloader)\n",
    "    grid.train_error[i] = trainer.callback_metrics[\"train_loss\"].item()\n",
    "    grid.test_error[i] = trainer.callback_metrics[\"val_loss\"].item()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "735aa66b4eab240e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grid.sort_values(by=\"test_error\").head(10)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "635fb8fc9f62f9e2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### We can see that the best parameters for a normal feed forward neural network are : \n",
    "- learning rate : 0.001\n",
    "- hidden layer width : 32\n",
    "- hidden layer depth : 8\n",
    "- epochs : 100\n",
    "- dropout : 0.2\n",
    "\n",
    "##"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8bc92d0ebc5470d9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using an autoencoder as a pretraining step for the MLP and check if the performance improves"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b8a64f5558102f5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model = MLP(input_dim=train.shape[1], output_dim=1, hidden_layers=[32]*7, lr=0.001, dropout=0.2)\n",
    "model = AutoEncoder(input_dim=train.shape[1], output_dim=473, hidden_layers=[32]*8, lr=0.001, dropout=0.2)\n",
    "trainer = Trainer(max_epochs=150, enable_progress_bar=False, enable_model_summary=False)\n",
    "trainer.fit(model, train_dataloader, test_dataloader)\n",
    "print(trainer.callback_metrics[\"train_loss\"].item(), trainer.callback_metrics[\"val_loss\"].item())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "52d199febb15d53"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "finetuned_model = MLP(input_dim=train.shape[1], output_dim=473, hidden_layers=[32]*8, lr=0.001, dropout=0.2)\n",
    "finetuned_model.load_state_dict(model.state_dict())\n",
    "trainer = Trainer(max_epochs=100, enable_progress_bar=True, enable_model_summary=False)\n",
    "finetuned_model.layers = finetuned_model.layers[:-1].append(nn.Linear(32,1))\n",
    "\n",
    "trainer.fit(finetuned_model, train_dataloader, test_dataloader)\n",
    "print(trainer.callback_metrics[\"train_loss\"].item(), trainer.callback_metrics[\"val_loss\"].item())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ccd2322bce06213"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unpretrained_model = MLP(input_dim=train.shape[1], output_dim=1, hidden_layers=[32]*8, lr=0.001, dropout=0.2)\n",
    "trainer = Trainer(max_epochs=100, enable_progress_bar=False, enable_model_summary=False)\n",
    "trainer.fit(unpretrained_model, train_dataloader, test_dataloader)\n",
    "print(trainer.callback_metrics[\"train_loss\"].item(), trainer.callback_metrics[\"val_loss\"].item())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "abe4da9fbd043c17"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### We can see that the autoencoder does not improve the performance of the MLP, and that the performance of the MLP is better than the linear regression"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4736c3cd5eda109a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using a DAE as a pretraining step for the MLP and check if the performance improves"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a96c046630aa058"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = DAE(input_dim=train.shape[1], output_dim=473, hidden_layers=[32]*8, noise_mean=0.1, noise_std=0.1, lr=0.001, dropout=0.2)\n",
    "trainer = Trainer(max_epochs=150, enable_progress_bar=False, enable_model_summary=False)\n",
    "trainer.fit(model, train_dataloader, test_dataloader)\n",
    "print(trainer.callback_metrics[\"train_loss\"].item(), trainer.callback_metrics[\"val_loss\"].item())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77b037a02a94ea2d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "finetuned_model = MLP(input_dim=train.shape[1], output_dim=473, hidden_layers=[32]*8, lr=0.001, dropout=0.2)\n",
    "finetuned_model.load_state_dict(model.state_dict())\n",
    "trainer = Trainer(max_epochs=100, enable_progress_bar=False, enable_model_summary=False)\n",
    "finetuned_model.layers = finetuned_model.layers[:-1].append(nn.Linear(32,1))\n",
    "trainer.fit(finetuned_model, train_dataloader, test_dataloader)\n",
    "print(trainer.callback_metrics[\"train_loss\"].item(), trainer.callback_metrics[\"val_loss\"].item())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dad6c2d2744b707f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using categorical embedding for the genre"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80ab86045b60eec3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "genres.loc[train_idx].values"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9c5e1530cb503b6"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# create test and train dataloader from the dataset\n",
    "train, test = train_no_genre.astype(np.float32), test_no_genre.astype(np.float32)\n",
    "train_dataset = CategoricalDataset(data=train, target=train_target, genre=genres.loc[train_idx].values)\n",
    "test_dataset =  CategoricalDataset(data=test, target=test_target, genre=genres.drop(train_idx).values)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=8)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=8)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T08:41:09.455176842Z",
     "start_time": "2023-10-11T08:41:09.288267521Z"
    }
   },
   "id": "311fe1d6f2099659"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# initialize mlp and create a gridsearch to find the best parameters recording test and train error for each combination of parameters\n",
    "lr = [0.001, 0.01, 0.1]\n",
    "hidden_layer_width = [32, 64, 128]\n",
    "hidden_layer_depth = [1, 2, 4, 8]\n",
    "embedding_dim = [16, 32, 64]\n",
    "epochs = [20, 50, 100]\n",
    "dropout = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "grid = np.array(np.meshgrid(lr, hidden_layer_width, hidden_layer_depth, embedding_dim, epochs, dropout)).T.reshape(-1, 6)\n",
    "grid = pd.DataFrame(grid, columns=[\"lr\", \"hidden_layer_width\", \"hidden_layer_depth\", \"embedding_dim\", \"epochs\", \"dropout\"])\n",
    "grid[\"train_error\"] = np.nan\n",
    "grid[\"test_error\"] = np.nan\n",
    "grid[[\"hidden_layer_width\", \"hidden_layer_depth\", \"embedding_dim\", \"epochs\"]] = grid[[\"hidden_layer_width\", \"hidden_layer_depth\", \"embedding_dim\", \"epochs\"]].astype(int)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aec6d6fbc8b60357"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# go through the grid and train the model for each combination of parameters\n",
    "for i in tqdm(range(len(grid))):\n",
    "    layers = [grid.hidden_layer_width[i]]*grid.hidden_layer_depth[i]\n",
    "    model = CategMLP(input_dim=train.shape[1], output_dim=1, categ_dim=genres.shape[1], embedding_dim=grid.embedding_dim[i], hidden_layers=layers,\n",
    "                     lr=grid.lr[i], dropout=grid.dropout[i], )\n",
    "    trainer = Trainer(max_epochs=int(grid.epochs[i]), enable_progress_bar=False, enable_model_summary=False)\n",
    "    trainer.fit(model, train_dataloader, test_dataloader)\n",
    "    grid.train_error[i] = trainer.callback_metrics[\"train_loss\"].item()\n",
    "    grid.test_error[i] = trainer.callback_metrics[\"val_loss\"].item()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e367d49f916a06"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "attention_mlp = AttentionMLP(input_dim=train.shape[1], output_dim=1, hidden_layers=[32]*8, lr=0.0001, dropout=0.5)\n",
    "trainer = Trainer(max_epochs=200, enable_progress_bar=False, enable_model_summary=False)\n",
    "trainer.fit(attention_mlp, train_dataloader, test_dataloader)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T09:26:58.915800316Z",
     "start_time": "2023-10-11T09:21:17.139310586Z"
    }
   },
   "id": "10a8a43e6fedcb61"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "(128.95205688476562, 3994.33349609375)"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.callback_metrics[\"train_loss\"].item(), trainer.callback_metrics[\"val_loss\"].item()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T09:26:58.957295747Z",
     "start_time": "2023-10-11T09:26:58.956339811Z"
    }
   },
   "id": "90d4a10324371789"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "{'train_loss': tensor(128.9521), 'val_loss': tensor(3994.3335)}"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.callback_metrics"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T09:27:51.888611129Z",
     "start_time": "2023-10-11T09:27:51.808842089Z"
    }
   },
   "id": "56a7fdae576f47c9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "40d3f335124a96f6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
